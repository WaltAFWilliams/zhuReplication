{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "import wfdb\n",
    "import ast\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import *\n",
    "from keras_radam import RAdam\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to memory constraints we can't load in entire dataset at once so we split into two halves\n",
    "data = pd.read_csv(\"ptbxl_database.csv\")\n",
    "halfway = len(data)//2\n",
    "data1, data2 = data[:halfway], data[halfway:]\n",
    "path = \"/home/walt/ml/ECG/data/\"\n",
    "sample_rate = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts ECG data from raw files into numpy array\n",
    "def load_data(df, sample_rate, path):\n",
    "    if sample_rate==100:\n",
    "        data = [wfdb.rdsamp(path+f) for f in df.filename_lr]\n",
    "    else:\n",
    "        data = [wfdb.rdsamp(path+f) for f in df.filename_hr]\n",
    "    data = np.array([signal for signal, key in data], dtype=np.float32)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in first half of training data\n",
    "X = load_data(data1, sample_rate, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract annotations\n",
    "Y = pd.read_csv(\"ptbxl_database.csv\", index_col=\"ecg_id\")\n",
    "Y.scp_codes = Y.scp_codes.apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scp_statements.csv for combining diagnoses\n",
    "desc = pd.read_csv(\"scp_statements.csv\", index_col=0)\n",
    "desc = desc[desc.diagnostic==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregating diagnoses (Converting diagnostic dictionaries into disease labels)\n",
    "def combine_diagnoses(y_dic):\n",
    "    tmp = []\n",
    "    for key in y_dic.keys():\n",
    "        if key in desc.index:\n",
    "            tmp.append(desc.loc[key].diagnostic_class)\n",
    "    return tmp\n",
    "Y[\"diagnostic_superclass\"] = Y.scp_codes.apply(combine_diagnoses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CD', 'HYP', 'MI', 'NORM', 'STTC'], dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One hot encoding labels\n",
    "labels = Y.diagnostic_superclass.values\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels = mlb.fit_transform(labels)\n",
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: (9826, 5000, 12)\n",
      "Train Labels Shape: (9826, 5)\n",
      "Test Data Shape: (1092, 5000, 12)\n",
      "Test Labels Shape: (1092, 5)\n"
     ]
    }
   ],
   "source": [
    "# Train test split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, labels[:halfway], test_size=0.1, random_state=69420)\n",
    "\n",
    "print(f\"Training Data Shape: {X_train.shape}\",\n",
    "     f\"Train Labels Shape: {y_train.shape}\",\n",
    "     f\"Test Data Shape: {X_valid.shape}\",\n",
    "     f\"Test Labels Shape: {y_valid.shape}\", sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Replication\n",
    "\n",
    "#### Paper can be found here https://www.thelancet.com/journals/landig/article/PIIS2589-7500(20)30107-2/fulltext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZhuModel(tf.keras.Model):\n",
    "    \"\"\"Changes in architecture\n",
    "    - Original Model: Conv filters = 64,128,256,512. \n",
    "    - This model: Conv filters = 32,64,128,256. \n",
    "    - Reason: Original model trained with ~400k samples. This dataset has ~22k samples so filter sizes of that size\n",
    "    will likely result in overfitting. \n",
    "    \n",
    "    - Original Model: Dense Layer sizes (512, 512, 21)\n",
    "    - This Model: Dense Layer sizes (256, 256, 5)\n",
    "    - Reason: Reduce parameters and reduce chance of overfitting. Also, original model was trained to detect 21 \n",
    "    separate classes whereas this one is detecting 5 classes due to the dataset it is being trained on.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ZhuModel, self).__init__()\n",
    "       \n",
    "        # Input block\n",
    "        self.conv1A = Conv1D(64, (15), 2, input_shape=X.shape[1:])\n",
    "        self.bn1A = BatchNormalization()\n",
    "        self.act1A = Activation(\"relu\")\n",
    "        self.pool1 = MaxPool1D(pool_size=2, strides=2)\n",
    "        \n",
    "        # 4 CONV + IDEN blocks\n",
    "        \n",
    "        # Conv Block 1\n",
    "        self.bl1conv1 = Conv1D(64, (15), 2)\n",
    "        self.bl1bn1 = BatchNormalization()\n",
    "        self.bl1act1 = Activation(\"relu\")\n",
    "        self.bl1conv2 = Conv1D(64, (15), 1, padding=\"same\")\n",
    "        self.bl1bn2 = BatchNormalization()\n",
    "        self.bl1act2 = Activation(\"relu\")\n",
    "        \n",
    "        self.bl1pool1 = MaxPool1D(2, strides=2)\n",
    "        \n",
    "        # Iden Block 1\n",
    "        self.bl1conv3 = Conv1D(64, (15), 1, padding=\"same\")\n",
    "        self.bl1bn3 = BatchNormalization()\n",
    "        self.bl1act3 = Activation(\"relu\")\n",
    "        \n",
    "        # Conv Block 2\n",
    "        self.bl2conv1 = Conv1D(128, (15), 2)\n",
    "        self.bl2bn1 = BatchNormalization()\n",
    "        self.bl2act1 = Activation(\"relu\")\n",
    "        self.bl2conv2 = Conv1D(128, (15), 1, padding=\"same\")\n",
    "        self.bl2bn2 = BatchNormalization()\n",
    "        self.bl2act2 = Activation(\"relu\")\n",
    "        self.bl2pool1 = MaxPool1D(2, strides=2)\n",
    "   \n",
    "        # Iden Block 2\n",
    "        self.bl2conv3 = Conv1D(128, (15), 1, padding=\"same\")\n",
    "        self.bl2bn3 = BatchNormalization()\n",
    "        self.bl2act3 = Activation(\"relu\")\n",
    "        \n",
    "        # Conv Block 3\n",
    "        self.bl3conv1 = Conv1D(256, (15), 2)\n",
    "        self.bl3bn1 = BatchNormalization()\n",
    "        self.bl3act1 = Activation(\"relu\")\n",
    "        self.bl3conv2 = Conv1D(256, (15), 1, padding=\"same\")\n",
    "        self.bl3bn2 = BatchNormalization()\n",
    "        self.bl3act2 = Activation(\"relu\")\n",
    "        \n",
    "        self.bl3pool1 = MaxPool1D(2, strides=2)\n",
    "        \n",
    "        # Iden Block 3\n",
    "        self.bl3conv3 = Conv1D(256, (15), 1, padding=\"same\")\n",
    "        self.bl3bn3 = BatchNormalization()\n",
    "        self.bl3act3 = Activation(\"relu\")\n",
    "        \n",
    "        # Conv Block 4\n",
    "        self.bl4conv1 = Conv1D(512, (15), 2)\n",
    "        self.bl4bn1 = BatchNormalization()\n",
    "        self.bl4act1 = Activation(\"relu\")\n",
    "        self.bl4conv2 = Conv1D(512, (15), 1, padding=\"same\")\n",
    "        self.bl4bn2 = BatchNormalization()\n",
    "        self.bl4act2 = Activation(\"relu\")\n",
    "        \n",
    "        self.bl4pool1 = MaxPool1D(2, strides=2)\n",
    "        \n",
    "        # Iden Block 4\n",
    "        self.bl4conv3 = Conv1D(512, (15), 1, padding=\"same\")\n",
    "        self.bl4bn3 = BatchNormalization()\n",
    "        self.bl4act3 = Activation(\"relu\")\n",
    "        \n",
    "        #self.pool2 = AveragePooling1D(2)\n",
    "        self.flatten = Flatten()\n",
    "        # DENSE BLOCKS\n",
    "        \n",
    "        # 2 Dense Blocks\n",
    "        self.fc1 = Dense(512)\n",
    "        self.fc1act = Activation(\"relu\")\n",
    "        self.drop1 = Dropout(0.6)\n",
    "        self.fc2 = Dense(512)\n",
    "        self.fc2act = Activation(\"relu\")\n",
    "        self.drop2 = Dropout(0.6)\n",
    "        \n",
    "        # Output layer\n",
    "        self.L = Dense(5, activation=\"sigmoid\")\n",
    "        \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Input convolution\n",
    "        x = self.conv1A(inputs)\n",
    "        x = self.bn1A(x)\n",
    "        x = self.act1A(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        # CONVBLOCK Pass 1\n",
    "        orig = x\n",
    "        orig = self.bl1conv1(orig)\n",
    "        orig = self.bl1bn1(orig)\n",
    "        \n",
    "        x = self.bl1conv1(x)\n",
    "        x = self.bl1bn1(x)\n",
    "        x = self.bl1act1(x)\n",
    "        x = self.bl1conv2(x)\n",
    "        x = self.bl1bn2(x)\n",
    "        x = self.bl1act2(x)\n",
    "        \n",
    "        # Add residual to original input\n",
    "        x += orig\n",
    "        x = self.bl1act3(x)\n",
    "        x = self.bl1pool1(x)\n",
    "        \n",
    "        # IDENBLOCK pass 1\n",
    "        orig = x\n",
    "        x = self.bl1conv3(x)\n",
    "        x = self.bl1bn3(x)\n",
    "        x = self.bl1act3(x)\n",
    "        x += orig\n",
    "        x = self.bl1act3(x)\n",
    "        \n",
    "        # CONVBLOCK Pass 2\n",
    "        orig = x\n",
    "        orig = self.bl2conv1(orig)\n",
    "        orig = self.bl2bn1(orig)\n",
    "        \n",
    "        x = self.bl2conv1(x)\n",
    "        x = self.bl2bn1(x)\n",
    "        x = self.bl2act1(x)\n",
    "        x = self.bl2conv2(x)\n",
    "        x = self.bl2bn2(x)\n",
    "        x = self.bl2act2(x)\n",
    "        \n",
    "        # Add residual to original input\n",
    "        x+=orig\n",
    "        x = self.bl2act3(x)\n",
    "        x = self.bl2pool1(x)\n",
    "        \n",
    "        # IDENBLOCK pass 2\n",
    "        orig = x\n",
    "        x = self.bl2conv3(x)\n",
    "        x = self.bl2bn3(x)\n",
    "        x = self.bl2act3(x)\n",
    "        x += orig\n",
    "        x = self.bl2act3(x)\n",
    "\n",
    "        # CONVBLOCK Pass 3\n",
    "        orig = x\n",
    "        orig = self.bl3conv1(orig)\n",
    "        orig = self.bl3bn1(orig)\n",
    "        \n",
    "        x = self.bl3conv1(x)\n",
    "        x = self.bl3bn1(x)\n",
    "        x = self.bl3act1(x)\n",
    "        x = self.bl3conv2(x)\n",
    "        x = self.bl3bn2(x)\n",
    "        x = self.bl3act2(x)\n",
    "        \n",
    "        # Add residual to original input\n",
    "        x+=orig\n",
    "        x = self.bl3act3(x)\n",
    "        x = self.bl3pool1(x)\n",
    "        \n",
    "        # IDENBLOCK pass 3\n",
    "        orig = x\n",
    "        x = self.bl3conv3(x)\n",
    "        x = self.bl3bn3(x)\n",
    "        x = self.bl3act3(x)\n",
    "        x += orig\n",
    "        x = self.bl3act3(x)\n",
    "\n",
    "        # CONVBLOCK Pass 4\n",
    "        orig = x\n",
    "        orig = self.bl4conv1(orig)\n",
    "        orig = self.bl4bn1(orig)\n",
    "        \n",
    "        x = self.bl4conv1(x)\n",
    "        x = self.bl4bn1(x)\n",
    "        x = self.bl4act1(x)\n",
    "        x = self.bl4conv2(x)\n",
    "        x = self.bl4bn2(x)\n",
    "        x = self.bl4act2(x)\n",
    "        \n",
    "        # Add residual to original input\n",
    "        x += orig\n",
    "        x = self.bl4act3(x)\n",
    "        #x = self.bl4pool1(x)\n",
    "        \n",
    "        # IDENBLOCK pass 4\n",
    "        orig = x\n",
    "        x = self.bl4conv3(x)\n",
    "        x = self.bl4bn3(x)\n",
    "        x = self.bl4act3(x)\n",
    "        x += orig\n",
    "        x = self.bl4act3(x)\n",
    "        \n",
    "        #x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc1act(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc2act(x)\n",
    "        x = self.drop2(x)\n",
    "        \n",
    "        # Output Layer\n",
    "        x = self.L(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 9528\n",
      "CD: 4907\n",
      "hyp 2655\n",
      "MI: 5486\n",
      "STTC: 5250\n"
     ]
    }
   ],
   "source": [
    "# Calculating class weights\n",
    "norm, cd, hyp, mi, sttc = [],[],[],[],[]\n",
    "def count_classes(x):\n",
    "    \n",
    "    if \"NORM\" in x: \n",
    "        norm.append(x)\n",
    "    if \"CD\" in x:\n",
    "        cd.append(x)\n",
    "    if \"HYP\" in x:\n",
    "        hyp.append(x)\n",
    "    if \"MI\" in x:\n",
    "        mi.append(x)\n",
    "    if \"STTC\" in x:\n",
    "        sttc.append(x)\n",
    "     \n",
    "Y.diagnostic_superclass.apply(lambda x: count_classes(x))    \n",
    "print(f\"Norm: {len(norm)}\", f\"CD: {len(cd)}\", f\"hyp {len(hyp)}\", f\"MI: {len(mi)}\", f\"STTC: {len(sttc)}\", sep=\"\\n\")\n",
    "weights = [1 - len(cls)/len(data) for cls in list([norm, cd, hyp, mi, sttc])]\n",
    "class_weights = {\n",
    "    0: weights[0],\n",
    "    1: weights[1],\n",
    "    2: weights[2],\n",
    "    3: weights[3],\n",
    "    4: weights[4]\n",
    "}\n",
    "class_weights\n",
    "\n",
    "# Creating learning rate schedule\n",
    "def exponential_decay(init_lr, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return init_lr * 0.1**(epoch/s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "exp_decay_fn = exponential_decay(0.01, 20)\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(exp_decay_fn)\n",
    "\n",
    "# Training parameters\n",
    "EPOCHS = 100\n",
    "BATCH = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=3, verbose=1)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0.005, patience=8)\n",
    "zhu = ZhuModel()\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "zhu.compile(optimizer=opt, loss=\"binary_crossentropy\", metrics=[\"categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "154/154 [==============================] - 15s 100ms/step - loss: 0.4040 - categorical_accuracy: 0.4455 - val_loss: 0.4694 - val_categorical_accuracy: 0.4982 - lr: 0.0100\n",
      "Epoch 2/100\n",
      "154/154 [==============================] - 14s 94ms/step - loss: 0.3098 - categorical_accuracy: 0.4814 - val_loss: 0.4613 - val_categorical_accuracy: 0.4863 - lr: 0.0089\n",
      "Epoch 3/100\n",
      "154/154 [==============================] - 14s 94ms/step - loss: 0.3004 - categorical_accuracy: 0.4894 - val_loss: 0.4195 - val_categorical_accuracy: 0.4918 - lr: 0.0079\n",
      "Epoch 4/100\n",
      "154/154 [==============================] - 14s 94ms/step - loss: 0.2888 - categorical_accuracy: 0.4957 - val_loss: 0.4060 - val_categorical_accuracy: 0.4936 - lr: 0.0071\n",
      "Epoch 5/100\n",
      "154/154 [==============================] - 14s 94ms/step - loss: 0.2844 - categorical_accuracy: 0.5056 - val_loss: 0.4049 - val_categorical_accuracy: 0.4808 - lr: 0.0063\n",
      "Epoch 6/100\n",
      "154/154 [==============================] - 15s 95ms/step - loss: 0.2812 - categorical_accuracy: 0.5000 - val_loss: 0.3937 - val_categorical_accuracy: 0.5064 - lr: 0.0056\n",
      "Epoch 7/100\n",
      "154/154 [==============================] - 15s 96ms/step - loss: 0.2752 - categorical_accuracy: 0.5008 - val_loss: 0.4034 - val_categorical_accuracy: 0.4908 - lr: 0.0050\n",
      "Epoch 8/100\n",
      "154/154 [==============================] - 15s 97ms/step - loss: 0.2729 - categorical_accuracy: 0.5064 - val_loss: 0.4485 - val_categorical_accuracy: 0.4780 - lr: 0.0045\n",
      "Epoch 9/100\n",
      "154/154 [==============================] - 15s 95ms/step - loss: 0.2707 - categorical_accuracy: 0.5142 - val_loss: 0.4088 - val_categorical_accuracy: 0.5147 - lr: 0.0040\n",
      "Epoch 10/100\n",
      "154/154 [==============================] - 15s 97ms/step - loss: 0.2674 - categorical_accuracy: 0.5338 - val_loss: 0.3749 - val_categorical_accuracy: 0.6062 - lr: 0.0035\n",
      "Epoch 11/100\n",
      "154/154 [==============================] - 15s 96ms/step - loss: 0.2591 - categorical_accuracy: 0.5747 - val_loss: 0.3789 - val_categorical_accuracy: 0.6383 - lr: 0.0032\n",
      "Epoch 12/100\n",
      "154/154 [==============================] - 15s 96ms/step - loss: 0.2548 - categorical_accuracy: 0.5925 - val_loss: 0.3660 - val_categorical_accuracy: 0.6456 - lr: 0.0028\n",
      "Epoch 13/100\n",
      "154/154 [==============================] - 15s 97ms/step - loss: 0.2473 - categorical_accuracy: 0.6071 - val_loss: 0.3648 - val_categorical_accuracy: 0.6419 - lr: 0.0025\n",
      "Epoch 14/100\n",
      "154/154 [==============================] - 15s 97ms/step - loss: 0.2444 - categorical_accuracy: 0.6062 - val_loss: 0.3821 - val_categorical_accuracy: 0.6200 - lr: 0.0022\n",
      "Epoch 15/100\n",
      "154/154 [==============================] - 15s 95ms/step - loss: 0.2383 - categorical_accuracy: 0.6243 - val_loss: 0.3521 - val_categorical_accuracy: 0.6392 - lr: 0.0020\n",
      "Epoch 16/100\n",
      "154/154 [==============================] - 15s 95ms/step - loss: 0.2333 - categorical_accuracy: 0.6377 - val_loss: 0.3699 - val_categorical_accuracy: 0.6566 - lr: 0.0018\n",
      "Epoch 17/100\n",
      "154/154 [==============================] - 15s 96ms/step - loss: 0.2280 - categorical_accuracy: 0.6506 - val_loss: 0.3640 - val_categorical_accuracy: 0.6383 - lr: 0.0016\n",
      "Epoch 18/100\n",
      "154/154 [==============================] - 15s 96ms/step - loss: 0.2251 - categorical_accuracy: 0.6552 - val_loss: 0.3519 - val_categorical_accuracy: 0.6016 - lr: 0.0014\n",
      "Epoch 19/100\n",
      " 14/154 [=>............................] - ETA: 11s - loss: 0.2162 - categorical_accuracy: 0.6752"
     ]
    }
   ],
   "source": [
    "H = zhu.fit(X_train, y_train, validation_data=(X_valid, y_valid), batch_size=BATCH, epochs=EPOCHS, \n",
    "            callbacks=[lr_schedule, early_stop], class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <tensorflow.python.keras.layers.pooling.MaxPooling1D object at 0x7fd73f4c37f0>, because it is not built.\n",
      "WARNING:tensorflow:From /home/walt/learn/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: zhu_first1/assets\n"
     ]
    }
   ],
   "source": [
    "# Saving model after training first half\n",
    "tf.keras.models.save_model(zhu, \"zhu_first1\", save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restarted notebook to load in second half of data so need to reload the model as well\n",
    "X = load_data(data2, sample_rate, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "zhu_final = tf.keras.models.load_model(\"zhu_first1\")\n",
    "zhu_final.optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: (9827, 5000, 12)\n",
      "Train Labels Shape: (9827, 5)\n",
      "Test Data Shape: (1092, 5000, 12)\n",
      "Test Labels Shape: (1092, 5)\n"
     ]
    }
   ],
   "source": [
    "# Train test split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, labels[halfway:], test_size=0.1, random_state=69420)\n",
    "\n",
    "print(f\"Training Data Shape: {X_train.shape}\",\n",
    "     f\"Train Labels Shape: {y_train.shape}\",\n",
    "     f\"Test Data Shape: {X_valid.shape}\",\n",
    "     f\"Test Labels Shape: {y_valid.shape}\", sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "154/154 [==============================] - 15s 99ms/step - loss: 0.2323 - categorical_accuracy: 0.6827 - val_loss: 0.3082 - val_categorical_accuracy: 0.6410 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "154/154 [==============================] - 14s 93ms/step - loss: 0.2060 - categorical_accuracy: 0.7022 - val_loss: 0.2915 - val_categorical_accuracy: 0.6658 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "154/154 [==============================] - 14s 94ms/step - loss: 0.1984 - categorical_accuracy: 0.7104 - val_loss: 0.2879 - val_categorical_accuracy: 0.6822 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "154/154 [==============================] - 14s 94ms/step - loss: 0.1893 - categorical_accuracy: 0.7227 - val_loss: 0.2931 - val_categorical_accuracy: 0.6603 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "154/154 [==============================] - 14s 94ms/step - loss: 0.1818 - categorical_accuracy: 0.7255 - val_loss: 0.2920 - val_categorical_accuracy: 0.6777 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "154/154 [==============================] - 15s 96ms/step - loss: 0.1708 - categorical_accuracy: 0.7378 - val_loss: 0.3008 - val_categorical_accuracy: 0.6712 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "154/154 [==============================] - 15s 96ms/step - loss: 0.1546 - categorical_accuracy: 0.7605 - val_loss: 0.3211 - val_categorical_accuracy: 0.6648 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.1373 - categorical_accuracy: 0.7760\n",
      "Epoch 00008: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "154/154 [==============================] - 15s 98ms/step - loss: 0.1373 - categorical_accuracy: 0.7760 - val_loss: 0.3604 - val_categorical_accuracy: 0.6511 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "154/154 [==============================] - 15s 98ms/step - loss: 0.1125 - categorical_accuracy: 0.8086 - val_loss: 0.3568 - val_categorical_accuracy: 0.6658 - lr: 1.0000e-05\n",
      "Epoch 10/100\n",
      "154/154 [==============================] - 15s 97ms/step - loss: 0.1069 - categorical_accuracy: 0.8185 - val_loss: 0.3685 - val_categorical_accuracy: 0.6667 - lr: 1.0000e-05\n",
      "Epoch 11/100\n",
      "154/154 [==============================] - 15s 97ms/step - loss: 0.1031 - categorical_accuracy: 0.8232 - val_loss: 0.3759 - val_categorical_accuracy: 0.6694 - lr: 1.0000e-05\n",
      "Epoch 12/100\n",
      "154/154 [==============================] - 15s 97ms/step - loss: 0.1009 - categorical_accuracy: 0.8178 - val_loss: 0.3852 - val_categorical_accuracy: 0.6658 - lr: 1.0000e-05\n",
      "Epoch 13/100\n",
      "154/154 [==============================] - ETA: 0s - loss: 0.0985 - categorical_accuracy: 0.8234\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "154/154 [==============================] - 15s 97ms/step - loss: 0.0985 - categorical_accuracy: 0.8234 - val_loss: 0.3950 - val_categorical_accuracy: 0.6612 - lr: 1.0000e-05\n",
      "Epoch 00013: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1eac07c160>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=5, factor=0.1, verbose=1)\n",
    "stop = tf.keras.callbacks.EarlyStopping(patience=10, monitor=\"val_loss\", verbose=1)\n",
    "zhu_final.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=EPOCHS, \n",
    "              batch_size=BATCH, class_weight=class_weights, callbacks=[reduce_lr, stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
